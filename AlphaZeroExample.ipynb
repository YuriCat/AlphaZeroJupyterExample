{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境構築\n",
    "# pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ゲームの実装 ここを他のドメインの実装に置き換えれば色々なゲームで動かせる\n",
    "# 高速な言語でこの部分のみ実装すると手軽に高速化出来る(C++ならpybind11やBoost.Pythonを利用可)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "BLACK, WHITE =1, -1 # 先手後手\n",
    "\n",
    "class State:\n",
    "    '''○×ゲームの盤面実装'''\n",
    "    X, Y = 'ABC',  '123'\n",
    "    C = {0: '_', BLACK: 'O', WHITE: 'X'}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((3, 3)) # (x, y)\n",
    "        self.color = 1\n",
    "        self.win_color = 0\n",
    "        self.record = []\n",
    "\n",
    "    def action2str(self, a):\n",
    "        return self.X[a // 3] + self.Y[a % 3]\n",
    "\n",
    "    def str2action(self, s):\n",
    "        return self.X.find(s[0]) * 3 + self.Y.find(s[1])\n",
    "\n",
    "    def record_string(self):\n",
    "        return ' '.join([self.action2str(a) for a in self.record])\n",
    "\n",
    "    def __str__(self):\n",
    "        # 表示\n",
    "        s = '   ' + ' '.join(self.Y) + '\\n'\n",
    "        for i in range(3):\n",
    "            s += self.X[i] + ' ' + ' '.join([self.C[self.board[i, j]] for j in range(3)]) + '\\n'\n",
    "        s += 'record = ' + self.record_string()\n",
    "        return s\n",
    "\n",
    "    def play(self, action):\n",
    "        # 行動で状態を進める関数\n",
    "        # action は board 上の位置 (0 ~ 8) または行動系列の文字列\n",
    "        if isinstance(action, str):\n",
    "            for astr in action.split():\n",
    "                self.play(self.str2action(astr))\n",
    "            return self\n",
    "\n",
    "        x, y = action // 3, action % 3\n",
    "        self.board[x, y] = self.color\n",
    "\n",
    "        # 3つ揃ったか調べる\n",
    "        if self.board[x, :].sum() == 3 * self.color \\\n",
    "          or self.board[:, y].sum() == 3 * self.color \\\n",
    "          or (x == y and np.diag(self.board, k=0).sum() == 3 * self.color) \\\n",
    "          or (x == 2 - y and np.diag(self.board[::-1,:], k=0).sum() == 3 * self.color):\n",
    "            self.win_color = self.color\n",
    "\n",
    "        self.color = -self.color\n",
    "        self.record.append(action)\n",
    "        return self\n",
    "\n",
    "    def terminal(self):\n",
    "        # 終端状態かどうか返す\n",
    "        return self.win_color != 0 or len(self.record) == 3 * 3\n",
    "\n",
    "    def terminal_reward(self):\n",
    "        # 終端状態での勝敗による報酬を返す\n",
    "        return self.win_color if self.color == BLACK else -self.win_color\n",
    "\n",
    "    def legal_actions(self):\n",
    "        # 可能な行動リストを返す\n",
    "        return [a for a in range(3 * 3) if self.board[a // 3, a % 3] == 0]\n",
    "\n",
    "    def action_length(self):\n",
    "        # 行動ラベルの総数(policyの出力サイズを決める)\n",
    "        return 3 * 3\n",
    "\n",
    "    def feature(self):\n",
    "        # ニューラルネットに入力する状態表現を返す\n",
    "        return np.stack([self.board == self.color, self.board == -self.color]).astype(np.float64)\n",
    "\n",
    "\n",
    "state = State().play('B1')\n",
    "print(state)\n",
    "print('input feature')\n",
    "print(state.feature())\n",
    "state = State().play('B2 A1 C2')\n",
    "print('input feature')\n",
    "print(state.feature())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ニューラルネットの実装(PyTorch)\n",
    "# AlphaZeroの論文のネットワーク構成を小さく再現\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBN(nn.Module):\n",
    "    def __init__(self, filters0, filters1, kernel_size):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(filters0, filters1, kernel_size, stride=1, padding=kernel_size//2, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(filters1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.bn(self.conv(x))\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, filters):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvBN(filters, filters, 3)\n",
    "        self.conv2 = ConvBN(filters, filters, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(x + self.conv2(F.relu(self.conv1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filters = 16\n",
    "num_blocks = 2\n",
    "\n",
    "class Net(nn.Module):\n",
    "    '''ニューラルネット計算を行うクラス'''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        state = State()\n",
    "        self.input_shape = state.feature().shape\n",
    "        self.board_size = self.input_shape[1] * self.input_shape[2]\n",
    "\n",
    "        self.layer0 = ConvBN(self.input_shape[0], num_filters, 3)\n",
    "        self.blocks = nn.ModuleList([ResidualBlock(num_filters) for _ in range(num_blocks)])\n",
    "        \n",
    "        self.conv_p = ConvBN(num_filters, 2, 1)\n",
    "        self.fc_p = nn.Linear(self.board_size * 2, state.action_length())\n",
    "        \n",
    "        self.conv_v = ConvBN(num_filters, 1, 1)\n",
    "        self.fc_v1 = nn.Linear(self.board_size * 1, 4)\n",
    "        self.fc_v2 = nn.Linear(4, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.layer0(x))\n",
    "        for block in self.blocks:\n",
    "            h = block(h)\n",
    "\n",
    "        h_p = F.relu(self.conv_p(h))\n",
    "        h_p = self.fc_p(h_p.view(-1, self.board_size * 2))\n",
    "\n",
    "        h_v = F.relu(self.conv_v(h))\n",
    "        h_v = F.relu(self.fc_v1(h_v.view(-1, self.board_size * 1)))\n",
    "        h_v = self.fc_v2(h_v)\n",
    "\n",
    "        # value(状態価値)にtanhを適用するので負け -1 ~ 勝ち 1\n",
    "        return F.softmax(h_p, dim=-1), torch.tanh(h_v)\n",
    "\n",
    "    def predict(self, state):\n",
    "        # 探索中に呼ばれる推論関数\n",
    "        self.eval()\n",
    "        x = torch.FloatTensor(state.feature()).view(-1, *self.input_shape)\n",
    "        with torch.no_grad():\n",
    "            p, v = self.forward(x)\n",
    "        return p.cpu().numpy()[0], v.cpu().numpy()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_net(net, state):\n",
    "    '''方策 (p)　と　状態価値 (v) を表示'''\n",
    "    print(state)\n",
    "    p, v = net.predict(state)\n",
    "    print('p = ')\n",
    "    print((p *1000).astype(int).reshape((-1, *net.input_shape[1:3])))\n",
    "    print('v = ', v)\n",
    "\n",
    "show_net(Net(), State())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モンテカルロ木探索の実装\n",
    "\n",
    "class Node:\n",
    "    '''ある1状態の探索結果を保存するクラス'''\n",
    "    def __init__(self, p, v):\n",
    "        self.p, self.v = p, v\n",
    "        self.n, self.q_sum = np.zeros_like(p), np.zeros_like(p)\n",
    "        self.n_all, self.q_sum_all = 1, v / 2 # 事前分布(見解が分かれる点)\n",
    "\n",
    "    def update(self, action, q_new):\n",
    "        # 行動のスタッツを更新\n",
    "        self.n[action] += 1\n",
    "        self.q_sum[action] += q_new\n",
    "\n",
    "        # ノード全体のスタッツも更新\n",
    "        self.n_all += 1\n",
    "        self.q_sum_all += q_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, copy\n",
    "\n",
    "class Tree:\n",
    "    '''探索木を保持してモンテカルロ木探索を行うクラス'''\n",
    "    def __init__(self, net):\n",
    "        self.net = net\n",
    "        self.nodes = {}\n",
    "    \n",
    "    def search(self, state, depth):\n",
    "        # 終端状態の場合は末端報酬を返す\n",
    "        if state.terminal():\n",
    "            return state.terminal_reward()\n",
    "\n",
    "        # まだ未到達の状態はニューラルネットを計算して推定価値を返す\n",
    "        key = str(state)\n",
    "        if key not in self.nodes:\n",
    "            p, v = self.net.predict(state)\n",
    "            self.nodes[key] = Node(p, v)\n",
    "            return v\n",
    "\n",
    "        # 到達済みの状態はバンディットで行動を選んで状態を進める\n",
    "        node = self.nodes[key]\n",
    "        p = node.p\n",
    "        if depth == 0:\n",
    "            # ルートノード(現局面)では方策にノイズを加える\n",
    "            p = 0.75 * p + 0.25 * np.random.dirichlet([0.1] * len(p))\n",
    "\n",
    "        best_action, best_ucb = None, -float('inf')\n",
    "        for action in state.legal_actions():\n",
    "            n, q_sum = 1 + node.n[action], node.q_sum_all / node.n_all + node.q_sum[action]\n",
    "            ucb = q_sum / n + 2.0 * node.p[action] * np.sqrt(node.n_all) / n # PUCBの式\n",
    "\n",
    "            if ucb > best_ucb:\n",
    "                best_action, best_ucb = action, ucb\n",
    "\n",
    "        # 一手進めて再帰で探索\n",
    "        state.play(best_action)\n",
    "        q_new = -self.search(state, depth + 1) # 1手ごとの手番交代を想定\n",
    "        node.update(best_action, q_new)\n",
    "\n",
    "        return q_new\n",
    "\n",
    "    def think(self, state, num_simulations, temperature = 0, show=False):\n",
    "        # 探索のエンドポイント\n",
    "        if show:\n",
    "            print(state)\n",
    "        start, prev_time = time.time(), 0\n",
    "        for _ in range(num_simulations):\n",
    "            self.search(copy.deepcopy(state), depth=0)\n",
    "\n",
    "            # 1秒ごとに探索結果を表示\n",
    "            if show:\n",
    "                tmp_time = time.time() - start\n",
    "                if int(tmp_time) > int(prev_time):\n",
    "                    prev_time = tmp_time\n",
    "                    root, pv = self.nodes[str(state)], self.pv(state)\n",
    "                    print('%.2f sec. best %s. q = %.4f. n = %d / %d. pv = %s'\n",
    "                          % (tmp_time, state.action2str(pv[0]), root.q_sum[pv[0]] / root.n[pv[0]],\n",
    "                             root.n[pv[0]], root.n_all, ' '.join([state.action2str(a) for a in pv])))\n",
    "\n",
    "        #  訪問回数で重みつけた確率分布を返す\n",
    "        root = self.nodes[str(state)]\n",
    "        n = (root.n / np.max(root.n)) ** (1 / (temperature + 1e-8))\n",
    "        return n / n.sum()\n",
    "        \n",
    "    def pv(self, state):\n",
    "        # 最善応手列（読み筋）を返す\n",
    "        s, pv_seq = copy.deepcopy(state), []\n",
    "        while True:\n",
    "            key = str(s)\n",
    "            if key not in self.nodes or self.nodes[key].n.sum() == 0:\n",
    "                break\n",
    "            best_action = sorted([(a, self.nodes[key].n[a]) for a in s.legal_actions()], key=lambda x: -x[1])[0][0]\n",
    "            pv_seq.append(best_action)\n",
    "            s.play(best_action)\n",
    "        return pv_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 初期ネットワークで探索を行う\n",
    "tree = Tree(Net())\n",
    "tree.think(State(), 1000, show=True)\n",
    "\n",
    "tree = Tree(Net())\n",
    "tree.think(State().play('A1 C1 A2 C2'), 10000, show=True)\n",
    "\n",
    "tree = Tree(Net())\n",
    "tree.think(State().play('B2 A2 A3 C1 B3'), 10000, show=True)\n",
    "\n",
    "tree = Tree(Net())\n",
    "tree.think(State().play('B2 A2 A3 C1'), 10000, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ニューラルネットの学習\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 40\n",
    "\n",
    "def gen_target(ep):\n",
    "    '''ニューラルネットの学習用 input, targets を生成'''\n",
    "    turn_idx = np.random.randint(len(ep[0]))\n",
    "    state = State()\n",
    "    for a in ep[0][:turn_idx]:\n",
    "        state.play(a)\n",
    "    return state.feature(), ep[1][turn_idx], [ep[2] if turn_idx % 2 == 0 else -ep[2]]\n",
    "\n",
    "def train(episodes):\n",
    "    net = Net()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=3e-3, weight_decay=1e-4, momentum=0.9)\n",
    "    for epoch in range(num_epochs):\n",
    "        p_loss_sum, v_loss_sum = 0, 0\n",
    "        net.train()\n",
    "        for i in range(0, len(episodes), batch_size):\n",
    "            x, p_target, v_target = zip(*[gen_target(episodes[np.random.randint(len(episodes))]) for j in range(batch_size)])\n",
    "            x = torch.FloatTensor(np.array(x))\n",
    "            p_target = torch.FloatTensor(np.array(p_target))\n",
    "            v_target = torch.FloatTensor(np.array(v_target))\n",
    "\n",
    "            p, v = net(x)\n",
    "            p_loss = torch.sum(-p_target * torch.log(p))\n",
    "            v_loss = torch.sum((v_target - v) ** 2)\n",
    "\n",
    "            p_loss_sum += p_loss.item()\n",
    "            v_loss_sum += v_loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            (p_loss + v_loss).backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= 0.85\n",
    "    print('p_loss %f v_loss %f' % (p_loss_sum / len(episodes), v_loss_sum / len(episodes)))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# AlphaZeroのアルゴリズムメイン\n",
    "\n",
    "num_games = 200\n",
    "num_train_steps = 20\n",
    "num_simulations = 80\n",
    "\n",
    "net = Net()\n",
    "episodes = []\n",
    "result_distribution = {1:0, 0:0, -1:0}\n",
    "for g in range(num_games):\n",
    "    # 1対戦のエピソード生成\n",
    "    record, p_targets = [], []\n",
    "    state = State()\n",
    "    tree = Tree(net)\n",
    "    temperature = 1.0 # 探索結果から方策のtargetを作るときの温度\n",
    "    while not state.terminal():\n",
    "        p_target = tree.think(state, num_simulations, temperature)\n",
    "        # 行動をランダムに選んで進める\n",
    "        action = np.random.choice(np.arange(len(p_target)), p=p_target)\n",
    "        state.play(action)\n",
    "        record.append(action)\n",
    "        p_targets.append(p_target)\n",
    "        temperature *= 0.8\n",
    "    # 先手視点の報酬\n",
    "    reward = state.terminal_reward() * (1 if len(record) % 2 == 0 else -1)\n",
    "    result_distribution[reward] += 1\n",
    "    episodes.append((record, p_targets, reward))\n",
    "    if g % num_train_steps == 0:\n",
    "        print('game ', end='')\n",
    "    print(g, ' ', end='')\n",
    "\n",
    "    # ニューラルネットの学習\n",
    "    if (g + 1) % num_train_steps == 0:\n",
    "        print(result_distribution)\n",
    "        net = train(episodes)\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ニューラルネットの出力を見てみる\n",
    "\n",
    "#　初期状態\n",
    "print('initial state')\n",
    "show_net(net, State())\n",
    "\n",
    "# 置けば勝ち\n",
    "print('WIN by put')\n",
    "show_net(net, State().play('A1 C1 A2 C2'))\n",
    "\n",
    "# ダブルリーチにされているので負け\n",
    "print('LOSE by opponent\\'s double reach')\n",
    "show_net(net, State().play('B2 A2 A3 C1 B3'))\n",
    "\n",
    "#　ダブルリーチにすれば勝ち\n",
    "print('WIN through double reach')\n",
    "show_net(net, State().play('B2 A2 A3 C1'))\n",
    "\n",
    "\n",
    "# 難問: A1に置けば次の手番でダブルリーチにできて勝ち\n",
    "print('strategic WIN by following double')\n",
    "show_net(net, State().play('B1 A3'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習済みモデルでの探索\n",
    "\n",
    "tree = Tree(net)\n",
    "tree.think(State(), 100000, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
